# Memoire

**Memoire** is a distributed replay memory for reinforcement learning. Industrial application of reinforcement learning usually requires large amount of computation, both for environemnt exploration and neural network training. Our goal is to make it easier to write high-performance distributed reinforcement learning algorithm.

## How it works

The distributed reinforcement learning platform consists of two types of workers: **Actors** and **Learners**.

![DistRL](/docs/imgs/DistRL.png "Framework of Distributed RL")

An actor is responsible for exploring the environment and generating data for the learners. In its mainloop, it works as
1. Get latest model(policy) from learners.
2. Act in the environment according to the current policy.
3. Put generated experience in the client side of replay memory.
4. Client push samples to the server.

An learner is responsible for updating the model with batch data. In its mainloop, it works as
1. Get batch of samples from the server side of replay memory.
2. Update model with batch samples, according to different algorithms.
3. Publish latest model to actors.

We can distribute actors and learners in clusters (CPU and GPU) to fully utilize heterogeneous computing resources.

|      | Actor | Learner |
|:----:|:-----:|:-------:|
|Computing resource| CPU | GPU |
|DNN operation | Forward | F/B |
|Numbers | ~300 | ~10 |
|Memory usage | ~10G | ~1G |
|Bandwidth usage | ~1G | ~20G |

The client side of the replay memory stores recent trajectories generated by the local actor.
The size of local replay memory is limited by total steps/transitions AND total episodes.
We provide 3 methods to **create** space for a new episode, **add** a transition to the current episode,
and **close** a terminated episode.
When an episode is closed, the TD-lambda return for each step is calculated automatically,
and its priority for sampling is updated.
We also provide a method to sampling current trajectories to form a cache, and push it to the learner.

The server side receives pushed caches from clients automatically.
When we need batch of samples for training, we can **get** a batch from these pushed caches with another phase of sampling.
The two phases of sampling at the client side and the server side is designed to be (roughly)
equivalent to sampling from the whole replay memory across actors.

A complete list of supported methods and options can be found at [API](docs/API.md) page.

Note that in this framework, only the sampled transitions instead of all the generated trajectories,
are pushed to the learner for model updating.
In the case that we have enormous number of actors,
this kind of design can decrease both the bandwidth burden and memory usage of the learner.
At the same time, the learner can still get the sample with high priority,
and update the model efficiently by the flavour of prioritized sampling.

## Features
+ Prioritized Sampling

  Prioritized experience replay [1] is a method of selecting high-priority samples for training. It is argubly the most effective technique for good performance of (distributed) reinforcement learning [2] [3].

+ Framework Independency

  The replay memory module is seperated from the training of neural network, thus making it independent of the deep learning framework used to implement the neural network (e.g. TensorFlow, PyTorch, etc.). We hope the modulized design can provide more flexibility for deep learning practitioners.

+ Frame Stacking, N-Step Learning, Multi-dimensional Reward, TD-lambda return computing.

  These are common and useful components of reinforcement learning in practice.

## Usage
See `example/`

## Build
```shell
cd build/
make
```

## Dependency
ZeroMQ, google-test, pybind11, libbfd (for debug)

## Documentation
(TODO) See source code

## TODO
+ Variable size of state

## Reference
+ [[1] T.Schaul et al. **Prioritized Experience Replay**](https://arxiv.org/abs/1511.05952)
+ [[2] M.Hessel et al. **Rainbow: Combining Improvements in Deep Reinforcement Learning**](https://arxiv.org/abs/1710.02298)
+ [[3] D.Horgan et al. **Distributed Prioritized Experience Replay**](https://arxiv.org/abs/1803.00933)

